---
title: "<strong>The Bootstrap</strong>"
author: "andrés castro araújo"
date: "`r format(Sys.Date(), '%b %d, %Y')`"
output: 
  html_document: 
    theme: paper
    toc: yes
    toc_float:
      collapsed: yes
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = "", fig.align = "center",
                      fig.width = 8, fig.height = 8)

library(tidyverse)

theme_custom <- function(base_line_size = 0.25) {
  theme_minimal(base_family = "IBM Plex Sans", base_line_size = base_line_size) +
    theme(
      plot.title = element_text(face = "bold"),
      panel.grid.minor = element_blank(),
      panel.grid.major = element_line(size = base_line_size),
      strip.background = element_rect(fill = "gray80", color = "gray80")
    ) 
}
```

```{css, echo=FALSE}
blockquote {
    padding: 10px 20px;
    margin: 0 0 20px;
    font-size: 14px;
    color: #828282;
    border-left: 10px solid #EEE;
}
body {
    font-size: 14px;
}
```


## Introduction

___Frequentist Uncertainty___

>If I were able to see a new sample of data, generated by the same processes and scenarios as my current data, how would my estimates change?

The sampling distribution  is the set of possible data sets that could have been observed, if the data collection process had been re-done. 

The true sampling distribution of a statistic $\hat \tau$ (or _estimator_) is usually unknowable, as it requires a large amount of different samples from which to compute the statistic.

```{r, fig.height=2, fig.width=6}
n <- 50
tau <- 2.5
S <- 1e4

sim <- replicate(S, rnorm(n, mean = tau, sd = 2), simplify = FALSE) %>% 
  map_dbl(mean)

tibble(sim) %>% 
  ggplot(aes(sim)) + geom_histogram(color = "black", bins = 30) + theme_custom() +
  labs(title = "The sampling distribution", 
       x = expression(hat(tau)), subtitle = expression(tau == 2.5),
       caption = 'This "bell shape" is explained by the Central Limit Theorem')
```

Traditionally, we use mathematical formulas to get "standard errors" of different estimators. The standard error is simply _the estimated standard deviation of the sampling distribution_.

_Mean estimator:_

$$
\overline X_n = \frac{1}{n} \sum_{i=1}^n 
$$

_Standard error:_

$$
SE\bigg(\overline X_n\bigg) = \frac{\sigma_X}{\sqrt{n}}
$$

The bootstrap _approximates_ the true sampling distribution of $\hat \tau$ by creating "new datasets" drawn from the observed sample. It is a method that can be applied to basically any estimator, regardless of how complicated it is.

```{r, fig.height=2, fig.width=6}
n <- 50
tau <- 2.5

observed_sample <- rnorm(n, mean = tau, sd = 2)

bootstrap_means <- replicate(1e4, {
  i <- sample(1:n, n, replace = TRUE)
  mean(observed_sample[i])
})

tibble::enframe(bootstrap_means) %>% 
  ggplot(aes(value)) +
  geom_histogram(aes(y = ..density..), bins = 30) + 
  stat_function(fun = dnorm, 
                args = list(
                  mean = mean(observed_sample), 
                  sd = sd(observed_sample) / sqrt(n))) +
  theme_custom() +
  labs(title = "The bootstrap and the theoretical sampling distribution",
       x = expression(hat(tau)))
```

This resampling of data is known as "bootstraping", based on the idea that the users of this procedure "pull themselves up by their own bootstraps" by obtaining an estimate of sampling distribution _without the need for any distributional assumptions or ever constructing a generative model for the data._

_How close do these approximations resemble the "truth"?_

```{r, fig.height=2, fig.width=6}
ggplot(NULL) +
  geom_density(data = enframe(sim), aes(value, fill = "sampling\ndistribution")) +
  geom_density(data = enframe(bootstrap_means), alpha = 0.5,
               aes(value, fill = "bootstrap\napproximation")) +
  theme_custom() + 
  labs(fill = NULL, x = expression(hat(tau)), 
       title = "The bootstrap and the real sampling distribution",
       caption = expression(n == 50),
       subtitle = "Larger samples and unbiased sampling makes these distributions converge") +
  theme(legend.position = "top")
```

## Using __`rsample`__

```{r, fig.height=2, fig.width=6}
library(rsample)

resample <- rsample::bootstraps(tibble(x = observed_sample), times = 1e4) 

glimpse(resample)

resample %>% 
  mutate(mean = map_dbl(splits, function(s) mean(rsample::analysis(s)$x))) %>% 
  ggplot(aes(mean)) +
  geom_histogram(color = "black", bins = 30) + 
  labs(x = expression(hat(tau))) + 
  theme_custom()
```

## The nonparametric bootstrap for confidence intervals

This algorithm "targets the distribution for errors (i.e. how much larger or smaller the estimates are than the target) rather than the distribution for the estimates themselves".

>The difference is that it targets the distribution for errors—how much larger or smaller the estimates are than the target—rather than the distribution for the estimates themselves. In this algorithm, the sample estimate.

We take the sample estimate $\hat \tau$ as the stand-in for "truth". If the boostraped $\hat \tau_b$ tend to be larger than $\hat \tau$, then we can assume that (on average) $\hat \tau$ is greater than $\tau$.

```{r}
n <- 30
observed_sample <- rnorm(n, mean = tau, sd = 2)

## observed standard deviation
sd(observed_sample) / (sqrt(n - 1))

## sampling distribution for sd\
sim <- replicate(S, rnorm(n, mean = tau, sd = 2), simplify = FALSE) %>% 
  map_dbl(mean)

sd(sim)

2 / sqrt(n - 1)

sim2 <- replicate(S, rnorm(n, mean = tau, sd = 2), simplify = FALSE) %>% 
  map_dbl(sd)

mean(sim2) / sqrt(n - 1)

#/ sqrt(n - 1))



bootstrap_sds <- replicate(1e4, {
  i <- sample(1:n, n, replace = TRUE)
  sd(observed_sample[i])
})

# Percent of bootstrap estimates that are lower than the "truth"
mean(bootstrap_sds > 2)

mean(bootstrap_sds > sd(observed_sample))
```

```{r}
n <- 30
observed_sample <- rnorm(n, mean = tau, sd = 2)

sim <- replicate(1e3, rnorm(n, mean = tau, sd = 2), simplify = FALSE) %>% 
  map_dbl(sd)

mean(sim / 2)

mean(sim < 2)

sd(observed_sample) / 2 ## sample sd is smaller than true sd

boostrap_sim <- replicate(1e3, {
  i <- sample(1:n, n, replace = TRUE)
  sd(observed_sample[i]) - sd(observed_sample)
})

mean(boostrap_sim)


sd(iris[iris$Species == "setosa", ]$Sepal.Length)

boostrap_sim <- replicate(1e3, {
  i <- sample(1:length(iris[iris$Species == "setosa", ]$Sepal.Length),
              length(iris[iris$Species == "setosa", ]$Sepal.Length), replace = TRUE)
  sd(iris[iris$Species == "setosa", ]$Sepal.Length[i]) - sd(iris[iris$Species == "setosa", ]$Sepal.Length)
})


mean(boostrap_sim)
```

```{r, eval=FALSE}
url <- "https://raw.githubusercontent.com/TaddyLab/BDS/master/chapters/1-Uncertainty/data/web-browsers.csv"

read_csv(url)
```


The sample variance is a biased estimate of the true population variance.

To do: 

- Butcher most of this and add a "frequentist uncertainty" notebook to Self Studying git hub.

- This will have 1. Bootstrap, 2. Bootstraping the residuals (more assumptions, but deals with bias), 3. Parametric bootstrap (Lasso example).

- Hypothesis testing and False Discovery Rates

$p$-value is a probability that represents how "rare" or "strange" your sample would be _if_ the null hypothesis where to be true. 

$p$-values stop being reliable due to multiple testing. 


|           | Decision to *accept* $H_0$  | Decision to *reject* $H_0$
|:----------|:---------------------------:|:---------------------------:
| $H_0$ is true   | Correct decision      | Type I error ($\alpha$)
| $H_0$ is false  | Type II error         | Correct decision
Table: **Statistical Testing Framework**



fdp

E(fdp)

Bradley Efron compares Byaesian inference to the parametric bootstrap.