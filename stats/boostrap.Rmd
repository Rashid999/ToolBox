---
title: "<strong>The Bootstrap</strong>"
author: "andrés castro araújo"
date: "`r format(Sys.Date(), '%b %d, %Y')`"
output: 
  html_document: 
    theme: paper
    toc: yes
    toc_float:
      collapsed: yes
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = "", fig.align = "center",
                      fig.width = 8, fig.height = 8)

library(tidyverse)

theme_custom <- function(base_line_size = 0.25) {
  theme_minimal(base_family = "IBM Plex Sans", base_line_size = base_line_size) +
    theme(
      plot.title = element_text(face = "bold"),
      panel.grid.minor = element_blank(),
      panel.grid.major = element_line(size = base_line_size),
      strip.background = element_rect(fill = "gray80", color = "gray80")
    ) 
}
```

```{css, echo=FALSE}
blockquote {
    padding: 10px 20px;
    margin: 0 0 20px;
    font-size: 14px;
    color: #828282;
    border-left: 10px solid #EEE;
}
body {
    font-size: 14px;
}
```


## Introduction

___Frequentist Uncertainty___

>If I were able to see a new sample of data, generated by the same processes and scenarios as my current data, how would my estimates change?

The true sampling distribution of a statistic $\hat \tau$ (or _estimator_) is usually unknowable, as it requires a large amount of different samples from which to compute the statistic.

```{r, fig.height=2, fig.width=6}
n <- 50
tau <- 2.5
S <- 1e4

sim <- replicate(S, rnorm(n, mean = tau, sd = 2), simplify = FALSE) %>% 
  map_dbl(mean)

tibble(sim) %>% 
  ggplot(aes(sim)) + geom_histogram(color = "black", bins = 30) + theme_custom() +
  labs(title = "The sampling distribution", 
       x = expression(hat(tau)), subtitle = expression(tau == 2.5),
       caption = 'This "bell shape" is explained by the Central Limit Theorem')
```

The bootstrap approximates the true sampling distribution of $\hat \tau$ by creating "new datasets" drawn from the observed sample. It is a method that can be applied to basically any statistic, no matter how complicated it is.

```{r, fig.height=2, fig.width=6}
n <- 50
tau <- 2.5

observed_sample <- rnorm(n, mean = tau, sd = 2)

bootstrap_means <- replicate(1e4, {
  i <- sample(1:n, n, replace = TRUE)
  mean(observed_sample[i])
})

tibble::enframe(bootstrap_means) %>% 
  ggplot(aes(value)) +
  geom_histogram(aes(y = ..density..), bins = 30) + 
  stat_function(fun = dnorm, 
                args = list(
                  mean = mean(observed_sample), 
                  sd = sd(observed_sample) / sqrt(n))) +
  theme_custom() +
  labs(title = "The bootstrap and the theoretical sampling distribution",
       x = expression(hat(tau)))
```

_How close do these approximations resemble the "truth"?_

```{r, fig.height=2, fig.width=6}
ggplot(NULL) +
  geom_density(data = enframe(sim), aes(value, fill = "sampling\ndistribution")) +
  geom_density(data = enframe(bootstrap_means), alpha = 0.5,
               aes(value, fill = "bootstrap\napproximation")) +
  theme_custom() + 
  labs(fill = NULL, x = expression(hat(tau)), 
       title = "The bootstrap and the real sampling distribution",
       caption = expression(n == 50),
       subtitle = "Larger samples and unbiased sampling makes these distributions converge") +
  theme(legend.position = "top")
```

## Using __`rsample`__

```{r, fig.height=2, fig.width=6}
library(rsample)

resample <- rsample::bootstraps(tibble(x = observed_sample), times = 1e4) 

glimpse(resample)

resample %>% 
  mutate(mean = map_dbl(splits, function(s) mean(rsample::analysis(s)$x))) %>% 
  ggplot(aes(mean)) +
  geom_histogram(color = "black", bins = 30) + 
  labs(x = expression(hat(tau))) + 
  theme_custom()
```

## The nonparametric bootstrap for confidence intervals

This algorithm "targets the distribution for errors (i.e. how much larger or smaller the estimates are than the target) rather than the distribution for the estimates themselves".

“The difference is that it targets the distribution for errors—how much larger or smaller the estimates are than the target—rather than the distribution for the estimates themselves. In this algorithm, the sample estimate”

We take the sample estimate $\hat \tau$ as the stand-in for "truth". If the boostraped $\hat \tau_b$ tend to be larger than $\hat \tau$, then we can assume that (on average) $\hat \tau$ is greater than $\tau$.

```{r}
n <- 30
observed_sample <- rnorm(n, mean = tau, sd = 2)

## observed standard deviation
sd(observed_sample) / (sqrt(n - 1))

## sampling distribution for sd\
sim <- replicate(S, rnorm(n, mean = tau, sd = 2), simplify = FALSE) %>% 
  map_dbl(mean)

sd(sim)

2 / sqrt(n - 1)

sim2 <- replicate(S, rnorm(n, mean = tau, sd = 2), simplify = FALSE) %>% 
  map_dbl(sd)

mean(sim2) / sqrt(n - 1)

#/ sqrt(n - 1))



bootstrap_sds <- replicate(1e4, {
  i <- sample(1:n, n, replace = TRUE)
  sd(observed_sample[i])
})

# Percent of bootstrap estimates that are lower than the "truth"
mean(bootstrap_sds > 2)

mean(bootstrap_sds > sd(observed_sample))
```

```{r}
n <- 30
observed_sample <- rnorm(n, mean = tau, sd = 2)

sim <- replicate(1e3, rnorm(n, mean = tau, sd = 2), simplify = FALSE) %>% 
  map_dbl(sd)

mean(sim / 2)

mean(sim < 2)

sd(observed_sample) / 2 ## sample sd is smaller than true sd

boostrap_sim <- replicate(1e3, {
  i <- sample(1:n, n, replace = TRUE)
  sd(observed_sample[i]) - sd(observed_sample)
})

mean(boostrap_sim)


sd(iris[iris$Species == "setosa", ]$Sepal.Length)

boostrap_sim <- replicate(1e3, {
  i <- sample(1:length(iris[iris$Species == "setosa", ]$Sepal.Length),
              length(iris[iris$Species == "setosa", ]$Sepal.Length), replace = TRUE)
  sd(iris[iris$Species == "setosa", ]$Sepal.Length[i]) - sd(iris[iris$Species == "setosa", ]$Sepal.Length)
})


mean(boostrap_sim)
```

```{r}
url <- "https://raw.githubusercontent.com/TaddyLab/BDS/master/chapters/1-Uncertainty/data/web-browsers.csv"

read_csv(url)
```



The sample variance is a biased estimate of the true population variance.