---
title: "<strong>Statistical Modeling with `RStanarm`</strong>"
output: 
  html_document:
    code_folding: show
    theme: paper
    toc: yes
    toc_float: 
      collapsed: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = "", fig.align = "center", 
                      comment = "", cache = TRUE)
```

```{css, echo=FALSE}
blockquote {
    padding: 10px 20px;
    margin: 0 0 20px;
    font-size: 14px;
    color: #828282;
    border-left: 10px solid #EEE;
}

body {
    font-size: 14px;
}
```

<p align="right"><font size="2">
See [___here___](http://mc-stan.org/rstanarm/) for the official documentation. <br> 
See [___here___](https://mc-stan.org/users/documentation/case-studies/pool-binary-trials-rstanarm.html) for a complete case study.
</font><p>

****

```{r, message=FALSE}
## Packages
library(tidyverse)
library(rstanarm)
options(mc.cores = parallel::detectCores())

## Plot settings
theme_set(
  theme_minimal(base_family = "Verdana", base_line_size = 0)
  )
```


## Introduction

- In frequentist inference, we care about the probability of observing some random sample, given some fixed parameters.

    $$p(\mathbf y \mid \boldsymbol \theta)$$

- In Bayesian inference, we care  about the probability of parameters, given some fixed data.

    $$p(\boldsymbol \theta \mid \mathbf y)$$

    Bayesian inference has two more  to include prior information, which helps you get more stable estimates and predictions, or to account for additional structure in the data. And uncertainty is characterized using simulations from probability distributions (instead of relying on standard errors or bootstrap approximations).

****

In this example we look at a generalized linear model for binary data, aka _logistic regression_. See this [vignette](https://mc-stan.org/rstanarm/articles/binomial.html) for more information.

## Frequentist

This is how the output of a logistic regression will look like: 

```{r}
data(wells, package = "rstanarm")
wells <- mutate(wells, dist = dist / 100) # dist is in 100 meters

logistic_mod <- glm(switch ~ dist, data = wells, family = binomial("logit"))

arm::display(logistic_mod)
```

The 95% confidence intervals for `dist` is as follows:

```{r}
cat("Lower:", -0.62 + qnorm(0.025) * 0.10, "\n")
cat("Upper:", -0.62 + qnorm(0.975) * 0.10)
```

```{r}
# Get predictions

df <- predict(logistic_mod, se.fit = TRUE) %>% 
  as.data.frame() %>% 
  mutate(
    pred = plogis(fit),
    lower = plogis(fit - 2 * se.fit),
    upper = plogis(fit + 2 * se.fit),
    dist = wells$dist
    ) 
  
## Visualize

jitter_points <- function(...) {
  geom_jitter(data = wells, mapping = aes(dist, switch), height = 0.03, alpha = 0.1)
}

df %>% 
  ggplot(aes(x = dist, y = pred)) + 
  geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.2) +
  geom_line() +
  jitter_points() + 
  scale_y_continuous(labels = seq(0, 1, 0.1), breaks = seq(0, 1, 0.1))
```

The confidence intervals for the prediction at 0 and 300 meters are as follows:

```{r}
cat("0 meters:", "\n")
cat("  Lower:", plogis(0.61 + qnorm(0.025) * 0.06 + (-0.8159964 * 0)), "\n")
cat("  Upper:", plogis(0.61 + qnorm(0.975) * 0.06 + (-0.4240036 * 0)), "\n")
cat("300 meters:", "\n")
cat("  Lower:", plogis(0.61 + qnorm(0.025) * 0.06 + (-0.8159964 * 3)), "\n")
cat("  Upper:", plogis(0.61 + qnorm(0.975) * 0.06 + (-0.4240036 * 3)), "\n")
```

## Bayesian

And this is how the output of the same regression, fitted through `rstanarm`, looks like:

```{r, results = "hide"}
logistic_bayes <- stan_glm(switch ~ dist, data = wells, family = binomial("logit"))
```

```{r}
logistic_bayes %>% 
  summary(digits = 2)
```

The `summary()` of this `stanreg` object is very different from the usual `lm` or `glm` models. 

- For now, we are mostly interested in the "Model Info" and "Estimates" sections. The first is self-explanatory, while the second contains summaries of the posterior distribution for the coefficients. Note that `sd` slightly corresponds to what we earlier saw as a standard error. This is not a typo. Recall that the standard error for a parameter is simply the standard deviation of its sampling distribution. The standard deviation of a parameter's posterior distribution captures similar information.

- The `mean_PPD` quantity is explained in the _Posterior Predictive Checks_ section.

- The "MCMC" diagnostics deserves its own notebook and is not discussed here.

The contents of this object include things that are usually unavailable for regular `lm()` or `glm()` objects. For example:

```{r}
logistic_bayes$coefficients
logistic_bayes$covmat
logistic_bayes$algorithm
```

More importantly, we can easily access the posterior distribution for the parameters we just fitted.

```{r, message=FALSE}
draws <- as_tibble(logistic_bayes)

draws %>% 
  pivot_longer(cols = everything(), names_to = "parameters", values_to = "draw") %>%
  group_by(parameters) %>%
  summarise(
    mean = mean(draw),
    sd = sd(draw) 
    )

draws %>%
  ggplot(aes(`(Intercept)`, dist)) +
  geom_point(alpha = 0.2, size = 1) 

draws %>% 
  pivot_longer(cols = everything(), names_to = "parameters", values_to = "draw") %>%
  ggplot(aes(y = parameters, x = draw, fill = parameters)) +
  ggridges::geom_density_ridges(show.legend = FALSE) +
  labs(y = NULL)
```

__`predict()`__

The are two types of predictions we can obtain from `rstanarm`:

1. `posterior_linpred()` yields simulations of the possible values of $\boldsymbol \eta = \alpha + \beta \mathbf x$, with variation coming exclusively from the posterior uncertainty of the coefficients.

2. `posterior_predict()` yields simulations of the possible values of $\mathbf y^\text{rep} = \alpha + \beta \mathbf x + \epsilon$. This is also known as the _posterior predictive distribution_ of the data.

The `posterior_predict()` function is simply a short hand for the following procedure:

```{r}
n_sims <- nrow(draws)  # 4,000 draws from the posterior distribution
n_obs <- nrow(wells)   # 3,020 observations

yrep <- array(NA, dim = c(n_sims, n_obs))

for (i in 1:n_obs) {
  eta <- plogis(draws$`(Intercept)` + draws$dist * wells$dist[i])
  yrep[ , i] <- rbinom(n = n_sims, size = 1, prob = eta)
}

dim(yrep)
```

We can calculate the Posterior Predictive Distribution for the mean of the data (or `mean_PPD`) by making some calculations on this matrix.

```{r, fig.height=3, fig.width=8}
p <- c(0.1, 0.25, 0.5, 0.75, 0.9)
df <- tibble(mean_PPD = rowMeans(yrep))
  
summarize(df, mean = mean(mean_PPD), sd = sd(mean_PPD)) %>% 
  round(2) 

quantile(df$mean_PPD, p) %>% 
  round(2)
```

```{r}
df <- tibble(dist = seq(0, 3.5, 0.01))
pred_mat <- posterior_linpred(logistic_bayes, newdata = df) %>% plogis()

bayes_lines <- function(index, i) {
  geom_line(
    data = mutate(df, fit = pred_mat[index[i], ]),
    mapping = aes(dist, fit), color = "steelblue1", alpha = 0.05
    )
}

n_lines <- 400

ggplot(wells) +
  geom_jitter(aes(x = dist, y = switch), height = 0.03, alpha = 0.1) + 
  map(1:n_lines, bayes_lines, index = sample(4000, n_lines))
```

Finally, you can access the default priors that were used in the model with the `prior_summary()` function.

```{r}
prior_summary(logistic_bayes)
```

These are weakly informative priors. See more information [here](http://mc-stan.org/rstanarm/reference/prior_summary.stanreg.html) and [here](http://mc-stan.org/rstanarm/reference/priors.html).

We can also see how much we have learned about our parameters by comparing the prior and posterior distributions:

```{r, results="hide"}
g <- posterior_vs_prior(logistic_bayes, 
                        group_by_parameter = TRUE,
                        facet_args = list(scales = "free"))
```

```{r, fig.width=8, fig.height=3}
g
```

Ultimately, these are very generic weakly informative priors. If we want to exert more control over them, we need to set up additional arguments, such as:

```
stan_glm(...
  prior_intercept = normal(0, 1, autoscale = FALSE),
  prior = student_t(df = 1, 0, 1, autoscale = FALSE),
  prior_aux = cauchy(autoscale = TRUE))
```

****

## MAP approximation

The "optimizing" algorithm produces a MAP estimate or a _normal approximation centered at the posterior mode._ The idea is to take the Bayesian posterior distribution, and instead of sampling from it, which can be slow when the number of regression coefficients is large, we use an optimization algorithm to find its mode, and then use the curvature of the posterior density to construct a normal approximation.

This normal approximation is not perfect, especially for small datasets where thereâ€™s necessarily more uncertainty in the inference. But it can be used for speeding computations in "big data" applications.

```{r, results="hide"}
logistic_map <- stan_glm(switch ~ dist, data = wells,
                         family = binomial("logit"),
                         prior = NULL,
                         prior_intercept = NULL,
                         mean_PPD = FALSE, 
                         algorithm = "optimizing")
```

```{r}
logistic_map %>%
  summary(digits = 2)
```

```{r}
prior_summary(map_mod)
```

## Diagnostic plots

```{r, message=FALSE, warning=FALSE}
library(bayesplot)
bayesplot_theme_set(theme_minimal(base_family = "Verdana"))
color_scheme_set("viridis")

```


```{r}
n_pars <- nuts_params(logistic_bayes)

logistic_bayes %>% 
  as.array() %>% 
  mcmc_trace(facet_args = list(ncol = 1), np = n_pars)
```

```{r}
logistic_bayes %>% 
  as.array() %>% 
  mcmc_pairs(off_diag_args = list(size = 1, alpha = 0.2), np = n_pars)
```

## Posterior Predictive Checks

__Brief overview of Posterior Predictive Checks (PPCs)__

The intuition is simple: *if the model fits, then replicated data generated under the model should look similar to observed data.* And any systematic differences between the simulations and the data indicate potential failings of the model.

- Let $y$ be the observed data and $\boldsymbol \theta$ be the vector of parameters (including all the hyperparameters if the model is hierarchical).

- We define $y^\text{rep}$ as the replicated data that could have been observed, or, to think predictively, as the data we would see tomorrow if the "experiment" (or "process") that produced $y$ today were replicated with the same generative model and the same value of $\boldsymbol \theta$ that produced the observed data.

- We distinguish between $y^\text{rep}$ and $\widetilde y$, our general notation for predictive outcomes: $\widetilde y$ is any future observable value or vector of observable quantities, whereas $y^\text{rep}$ is *specifically a replication* just like $y$. For example, if the model has explanatory variables, $x$, they will be identical for $y$ and $y^\text{rep}$, but $\widetilde y$ may have its own explanatory variables, $\widetilde x$.

$$
p(\mathbf y^\text{rep} \mid \mathbf y) = \int p(\mathbf y^\text{rep} \mid \boldsymbol \theta) p(\boldsymbol \theta \mid \mathbf y)d\boldsymbol \theta
$$

- We define **test quantities** as the aspects of the data we wish to check (e.g. the *mean*, certain *quantiles*, outliers through *min* and *max*, and so forth). A test quantity (or *discrepancy measure*) $T(y, \theta)$ is a scalar summary of parameters *and* data used as a standard for comparing data to predictive simulations. Test quantities play the role in Bayesian model checking that test statistics play in classical testing. Thus, we use the notation $T(y)$ for classical *test statistics*.

```{r, fig.width=8, fig.height=2, message=FALSE}
color_scheme_set("blue")
ppc_stat(y = wells$switch, 
         yrep = posterior_predict(logistic_bayes), 
         stat = sd)

ppc_stat(y = wells$switch, 
         yrep = posterior_predict(logistic_bayes), 
         stat = mean)

prop_zero <- function(x) mean(x == 0)

ppc_stat(y = wells$switch, 
         yrep = posterior_predict(logistic_bayes), 
         stat = prop_zero)
```

## Model Comparison

To do...
